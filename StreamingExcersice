package com.bdec.kafka_java;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.concurrent.TimeoutException;

public class StreamingStructuredStreamWC {
    public static void main(String[] args) throws TimeoutException {

        String winutilPath = "C:\\Users\\shash\\Downloads\\winutils-master\\winutils-master\\hadoop-3.3.5";
        if(System.getProperty("os.name").toLowerCase().contains("win")) {
            System.out.println("Detected windows");
            System.setProperty("hadoop.home.dir", winutilPath);
            System.setProperty("HADOOP_HOME", winutilPath);
        }

        SparkSession spark = SparkSession
                .builder()
                .master("local[*]")
                .appName("SocgenJavaStructuredStreaming")
                .getOrCreate();

        spark.conf().set("spark.sql.shuffle.partitions", "2");

                StructType returnSchema = DataTypes.createStructType(
                new StructField[]{
                        DataTypes.createStructField("ID", DataTypes.IntegerType, false),
                        DataTypes.createStructField("wordInput", DataTypes.StringType, false)
                });

        Dataset<Row> positiveWordType = spark.read()
                .option("header", "false")
                .schema(returnSchema)
                .csv("file:///C:\\Users\\shash\\OneDrive\\Desktop\\TestProject\\src\\main\\resources\\Exsercies\\positive words.csv");

        Dataset<Row> negativeWordType = spark.read()
                .option("header", "false")
                .schema(returnSchema)
                .csv("file:///C:\\Users\\shash\\OneDrive\\Desktop\\TestProject\\src\\main\\resources\\Exsercies\\negative words.csv");

        Dataset<Row> streamingFiles = spark.readStream().text("file:///C:\\Users\\shash\\OneDrive\\Desktop\\TestProject\\src\\main\\resources\\Conversion");

        Dataset<Row> words = streamingFiles.select(functions.explode(
                functions.split(streamingFiles.col("value"), " "))
                .alias("word"));

        Dataset<Row> joinedPositiveWords = words.join(positiveWordType,
                words.col("word").equalTo(positiveWordType.col("wordInput"))).withColumn("WORD_TYPE", functions.lit("POSITIVE"));

        Dataset<Row> joinedNegativeWordType = words.join(negativeWordType,
                words.col("word").equalTo(negativeWordType.col("wordInput"))).withColumn("WORD_TYPE", functions.lit("NEGATIVE"));;

        Dataset<Row>  result =  joinedPositiveWords.union(joinedNegativeWordType);
        result = result.groupBy("WORD_TYPE").count();

        result =  result.withColumn("TEMPCOl", result.col("count"))
                .orderBy(functions.desc("TEMPCOl"))
                .withColumn("Document_type" , functions.col("WORD_TYPE"));

  //      result =  result.withColumn("DOCUMENT_TYPE" , result.first().getAs("TEMPCOl") )
                //.first().getAs("TEMPCOl");
//                .withColumn("documentType",functions.when(
//                        postiveWordCount.col("count").$less(NegativeWordCount.col("count"))
//                        ,functions.lit("POSITIVE_DOCUMENT")).otherwise(functions.lit("Negative_DOCUMENT")))
        ;
        StreamingQuery query = result.writeStream()
                .outputMode("complete")
                //.outputMode("append")
                .format("console").start();
        try {
            query.awaitTermination(100000);
        } catch (StreamingQueryException e) {
            e.printStackTrace();
        }
    }

}
