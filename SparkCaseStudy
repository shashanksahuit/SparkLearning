package com.bdec.training.javasparkl2;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.*;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.functions.*;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.functions.*;

import java.util.Arrays;

public class DBSpark {
    public static void main(String[] args) {
//        String sourcePath = args[0];
//        String targetTable = args[1];

        String sourcePath =   "C:\\Users\\shash\\OneDrive\\Desktop\\TestProject\\src\\main\\resources\\Global Superstore Sales - Global Superstore Sales.csv";
        String sourceReturnsPath =   "C:\\Users\\shash\\OneDrive\\Desktop\\TestProject\\src\\main\\resources\\Global Superstore Sales - Global Superstore Returns.csv";
        //String targetTable = args[1];
        SparkSession spark = SparkSession.builder()
                .appName("DBTraining")
                .master("local[*]")
                .getOrCreate();
       // spark.conf().set("spark.sql.legacy.timeParserPolicy","LEGACY");

        Dataset<Row> dfSrc = spark.read().format("CSV")
                .option("header", "true")
                .option("inferSchema", "true")
                .load(sourcePath);

        Dataset<Row> dfReturns = spark.read().format("CSV")
                .option("header", "true")
                .option("inferSchema", "true")
                .load(sourceReturnsPath);


        dfReturns.printSchema();

        System.out.println(dfSrc.count());
        Dataset<Row> joinedDfAfterReturnExcluded =   dfSrc.join(functions.broadcast(dfReturns),dfSrc.col("Order ID").notEqual(dfReturns.col("Order ID")),"leftsemi")
                ;

        Dataset<Row>  joinedDfWithYearAndMonth = joinedDfAfterReturnExcluded.toDF()
                .withColumn("DateColumnwithDatatype",functions.to_date(functions.col("Order-Date"),"M/d/yyyy"))
                .withColumn("year",functions.date_format(functions.col("DateColumnwithDatatype"),"yyyy").cast(DataTypes.IntegerType))
                .withColumn("month",functions.date_format(functions.col("DateColumnwithDatatype"),"M").cast(DataTypes.IntegerType))
                .withColumn("Profit_double", functions.regexp_replace(functions.col("Profit"), "[$,]", "").cast("double"))
                ;


        System.out.println(joinedDfAfterReturnExcluded.count());
        joinedDfAfterReturnExcluded.show(1000, false);
//        Dataset<Row>  joinedDfWithYearAndMonth = dfSrc.toDF()
//                .withColumn("DateColumnwithDatatype",functions.to_date(functions.col("Order-Date"),"M/d/yyyy"))
//                .withColumn("year",functions.date_format(functions.to_date(functions.col("Order-Date"),"m/d/yyyy"),"yyyy"))
//                .filter(functions.upper(functions.col("Returns")).equalTo("NO"))
//                ;
//        Dataset<Row>  joinedDfWithYearAndMonth = dfSrc.toDF().withColumn("DateColumnwithDatatype",functions.to_date(functions.col("Order-Date"),"M/d/yyyy"))
//                .filter(functions.upper(functions.col("Returns")).equalTo("NO"))
//                ;



//        org.apache.spark.sql.expressions.WindowSpec windowSpecAgg  = Window.partitionBy(joinedDfWithYearAndMonth.col("year"),
//                joinedDfWithYearAndMonth.col("month"),joinedDfWithYearAndMonth.col("Category"),joinedDfWithYearAndMonth.col("Sub-Category"));
//        joinedDfWithYearAndMonth =joinedDfWithYearAndMonth.toDF()
//                .withColumn("Profit_double", functions.regexp_replace(functions.col("Profit"), "[$,]", "").cast("double"))
//                .withColumn("Total Quantity Sold", functions.sum(functions.col("Quantity")).over(windowSpecAgg))
//                .withColumn("Total Profit", functions.sum(functions.col("Profit_double")).over(windowSpecAgg));




        joinedDfWithYearAndMonth = joinedDfWithYearAndMonth.toDF().groupBy(functions.col("year"),functions.col("month"),
                functions.col("Category"),
                functions.col("Sub-Category")
                ).agg(functions.sum(functions.col("Quantity")).alias("Total Quantity Sold"),
                functions.sum(functions.col("Profit_double")).alias("Total Profit"))
                .sort(functions.col("year"),functions.col("month"))

        ;



          joinedDfWithYearAndMonth.show(1000, false);


        //Dataset<Row> dfN = dfSrc.select("neighbourhood_group").distinct();
        //dfN.write().mode(SaveMode.Overwrite).saveAsTable(targetTable);
    }
}
